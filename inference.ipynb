{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ca1193b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from diffusers import UNet2DModel, DDPMScheduler, DiffusionPipeline\n",
    "from diffusers.training_utils import EMAModel\n",
    "from accelerate import Accelerator\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import soundfile as sf\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import librosa\n",
    "import seaborn as sns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e67f70e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from diffusers import UNet2DModel, DDPMScheduler, DDPMPipeline\n",
    "import json\n",
    "\n",
    "# Paths to your files\n",
    "ckpt_path           = \"ddpm-spectogram-256/unet/diffusion_pytorch_model.bin\"\n",
    "config_path         = \"ddpm-spectogram-256/unet\\config.json\"\n",
    "scheduler_config    = \"ddpm-spectogram-256/scheduler/scheduler_config.json\"\n",
    "\n",
    "# 1) Load model config & instantiate UNet2DModel\n",
    "with open(config_path, \"r\") as f:\n",
    "    unet_cfg = json.load(f)\n",
    "unet = UNet2DModel(**unet_cfg)\n",
    "\n",
    "# 2) Load scheduler config & instantiate DDPMScheduler\n",
    "with open(scheduler_config, \"r\") as f:\n",
    "    sched_cfg = json.load(f)\n",
    "scheduler = DDPMScheduler(**sched_cfg)\n",
    "\n",
    "# 3) Load weights into UNet\n",
    "state_dict = torch.load(ckpt_path, map_location=\"cpu\")\n",
    "unet.load_state_dict(state_dict)\n",
    "\n",
    "# 4) Assemble pipeline\n",
    "pipeline = DDPMPipeline(unet=unet, scheduler=scheduler)\n",
    "pipeline.to(\"cuda\")   # or \"cpu\"\n",
    "pipeline.unet.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fd4e2d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# Configuration\n",
    "num_samples = 10\n",
    "batch_size  = 2\n",
    "steps       = 1000\n",
    "output_dir  = Path(\"generated\")\n",
    "\n",
    "# Make sure output directory exists\n",
    "output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Find existing sample files and determine next index\n",
    "existing = list(output_dir.glob(\"sample_*.png\"))\n",
    "if existing:\n",
    "    # extract the numeric part from filenames like \"sample_005.png\"\n",
    "    existing_idxs = [\n",
    "        int(p.stem.split(\"_\")[1])\n",
    "        for p in existing\n",
    "        if p.stem.split(\"_\")[1].isdigit()\n",
    "    ]\n",
    "    next_idx = max(existing_idxs) + 1\n",
    "else:\n",
    "    next_idx = 0\n",
    "\n",
    "# Generate images\n",
    "all_images = []\n",
    "for _ in range(num_samples // batch_size):\n",
    "    images = pipeline(batch_size=batch_size, num_inference_steps=steps).images\n",
    "    all_images.extend(images)\n",
    "\n",
    "# Save with non-colliding filenames\n",
    "for i, img in enumerate(all_images):\n",
    "    idx = next_idx + i\n",
    "    filename = output_dir / f\"sample_{idx:03d}.png\"\n",
    "    img.save(filename)\n",
    "\n",
    "print(f\"Saved {len(all_images)} images, starting at index {next_idx:03d}.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dadacad0",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "pipe = DiffusionPipeline.from_pretrained(\"teticio/audio-diffusion-instrumental-hiphop-256\").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6e68e64",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "im = Image.open(r\"generated/sample_012.png\")\n",
    "a = pipe.mel.image_to_audio(im)\n",
    "from IPython.display import Audio\n",
    "display(Audio(a, rate=22100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17efc6c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "im2 = Image.open(r\"generated/sample_003.png\")\n",
    "a2 = pipe.mel.image_to_audio(im)\n",
    "from IPython.display import Audio\n",
    "display(Audio(a2, rate=22100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9cd2b10",
   "metadata": {},
   "outputs": [],
   "source": [
    "im"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da53ab2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "sr = 22100\n",
    "# Plot the original and reconstructed signals\n",
    "plt.figure(figsize=(12, 8))\n",
    "\n",
    "plt.subplot(2, 1, 1)\n",
    "librosa.display.waveshow(a, sr=sr, alpha=0.5)\n",
    "plt.title(\"Original Audio\")\n",
    "plt.xlabel(\"Time\")\n",
    "plt.ylabel(\"Amplitude\")\n",
    "\n",
    "plt.subplot(2, 1, 2)\n",
    "librosa.display.waveshow(a2, sr=sr, color='r', alpha=0.5)\n",
    "plt.title(\"Reconstructed Audio\")\n",
    "plt.xlabel(\"Time\")\n",
    "plt.ylabel(\"Amplitude\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5625569c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aud",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
